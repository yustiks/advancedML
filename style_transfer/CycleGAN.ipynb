{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "from model_builder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Styles for transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    0 : 'Gorodets',\n",
    "    1 : 'Gzhel',\n",
    "    2 : 'Iznik',\n",
    "    3 : 'Khokhloma',\n",
    "    4 : 'Neglyubka',\n",
    "    5 : 'Wycinanki_Å‚owickie',\n",
    "    6 : 'Wzory_kaszubskie'\n",
    "}\n",
    "\n",
    "style_X = styles[1]\n",
    "style_Y = styles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', style_X + ' =|= ' + style_Y, restoring_name)\n",
    "saving_path = os.path.join('models', style_X + ' =|= ' + style_Y, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1\n",
    "IMG_SIZE = 150\n",
    "\n",
    "LAMBDA = 5\n",
    "GEN_STEPS = 1\n",
    "DSC_STEPS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "            \n",
    "    # load training data from input queues     \n",
    "    X = inputs(style_X, BATCH_SIZE, EPOCHS)\n",
    "    Y = inputs(style_Y, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # normalize the images     \n",
    "    X = tf.div(tf.cast(X, tf.float32), 255.0)\n",
    "    Y = tf.div(tf.cast(Y, tf.float32), 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ G(X) -> Y ]==================#\n",
    "    \n",
    "    G_x = generator(X, 'X')\n",
    "    \n",
    "    #==================[ F(Y) -> X ]==================#   \n",
    "    \n",
    "    F_y = generator(Y, 'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Dy ]==================#\n",
    "    \n",
    "    dsc_Y = discriminator(Y, 'Y')\n",
    "    dsc_Fake_Y = discriminator(G_x, 'Y')\n",
    "    \n",
    "    #==================[ Dx ]==================#\n",
    "    \n",
    "    dsc_X = discriminator(X, 'X')\n",
    "    dsc_Fake_X = discriminator(F_y, 'X')\n",
    "    \n",
    "    #================[ Cyclic ]================#\n",
    "    \n",
    "    cyc_X = generator(G_x, 'Y')\n",
    "    cyc_Y = generator(F_y, 'X')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Adversarial Loss ]==================#\n",
    "    \n",
    "    # discriminators\n",
    "    L_DSC_X = tf.reduce_mean(tf.squared_difference(dsc_X, 1) + tf.square(dsc_Fake_X))\n",
    "    L_DSC_Y = tf.reduce_mean(tf.squared_difference(dsc_Y, 1) + tf.square(dsc_Fake_Y))\n",
    "    \n",
    "    # generators\n",
    "    L_GEN_X = tf.reduce_mean(tf.squared_difference(dsc_Fake_Y, 1))\n",
    "    L_GEN_Y = tf.reduce_mean(tf.squared_difference(dsc_Fake_X, 1))\n",
    "    \n",
    "    #==================[ Consistency Loss ]==================#\n",
    "    \n",
    "    L_CYC = tf.reduce_mean(tf.abs(X - cyc_X)) + tf.reduce_mean(tf.abs(Y - cyc_Y))\n",
    "    \n",
    "    #=====================[ Final Loss ]=====================#\n",
    "    \n",
    "    Loss_Gen_X = L_GEN_X + LAMBDA * L_CYC\n",
    "    Loss_Gen_Y = L_GEN_Y + LAMBDA * L_CYC\n",
    "    Loss_Dsc_X = L_DSC_X\n",
    "    Loss_Dsc_Y = L_DSC_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "    \n",
    "    gen_X_op = optimizer.minimize(Loss_Gen_X, var_list=tf.get_collection(\"GEN_X\"))\n",
    "    gen_Y_op = optimizer.minimize(Loss_Gen_Y, var_list=tf.get_collection(\"GEN_Y\"))\n",
    "    dsc_X_op = optimizer.minimize(Loss_Dsc_X, var_list=tf.get_collection(\"DSC_X\"))\n",
    "    dsc_Y_op = optimizer.minimize(Loss_Dsc_Y, var_list=tf.get_collection(\"DSC_Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorboard old data\n",
    "try:\n",
    "    shutil.rmtree('tensorboard')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "        \n",
    "    loss_gen_x = tf.summary.scalar('loss_gen_x', Loss_Gen_X)\n",
    "    loss_gen_y = tf.summary.scalar('loss_gen_y', Loss_Gen_Y)\n",
    "    loss_dsc_x = tf.summary.scalar('loss_dsc_x', Loss_Dsc_X)\n",
    "    loss_dsc_y = tf.summary.scalar('loss_dsc_y', Loss_Dsc_Y)\n",
    "    \n",
    "    x_original = tf.summary.image('X_original', X)\n",
    "    y_fake = tf.summary.image('Y_fake', G_x)\n",
    "    y_original = tf.summary.image('Y_original', Y)\n",
    "    x_fake = tf.summary.image('X_fake', F_y)\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('tensorboard', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    step = 0   \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "        step += 1\n",
    "        \n",
    "        # train generator X\n",
    "        for i in range(GEN_STEPS):\n",
    "            _, loss = sess.run([gen_X_op, Loss_Gen_X])\n",
    "            print(\"Step {0:5d} | Generator Y     {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))    \n",
    "#         # train discriminator Y\n",
    "#         for i in range(DSC_STEPS):\n",
    "#             _, loss = sess.run([dsc_Y_op, Loss_Dsc_Y])\n",
    "#             print(\"Step {0:5d} | Discriminator Y {1:5d} | loss = {2:6.3f}\".format(\n",
    "#                     step, i, loss)) \n",
    "        \n",
    "        # train generator Y\n",
    "        for i in range(GEN_STEPS):\n",
    "            _, loss, summary = sess.run([gen_Y_op, Loss_Gen_Y, merged])\n",
    "            print(\"Step {0:5d} | Generator X     {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))    \n",
    "#         # train discriminator X\n",
    "#         for i in range(DSC_STEPS):\n",
    "#             _, loss, summary = sess.run([dsc_X_op, Loss_Dsc_X, merged])\n",
    "#             print(\"Step {0:5d} | Discriminator X {1:5d} | loss = {2:6.3f}\".format(\n",
    "#                     step, i, loss)) \n",
    "            \n",
    "        # save stats to log         \n",
    "        summary_writer.add_summary(summary, step)\n",
    "                \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
