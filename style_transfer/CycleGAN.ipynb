{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Styles for transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    0 : 'Gorodets',\n",
    "    1 : 'Gzhel',\n",
    "    2 : 'Iznik',\n",
    "    3 : 'Khokhloma',\n",
    "    4 : 'Neglyubka',\n",
    "    5 : 'Wycinanki_Å‚owickie',\n",
    "    6 : 'Wzory_kaszubskie'\n",
    "}\n",
    "\n",
    "style_X = styles[1]\n",
    "style_Y = styles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', style_X + ' =|= ' + style_Y, restoring_name)\n",
    "saving_path = os.path.join('models', style_X + ' =|= ' + style_Y, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 150\n",
    "\n",
    "LAMBDA = 0\n",
    "GEN_STEPS = 25\n",
    "DSC_STEPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, stride, name, collection, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # add parameters to the collection\n",
    "        tf.add_to_collection(collection, W)\n",
    "        tf.add_to_collection(collection, b)\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, stride, stride, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        elif activation == \"sigmoid\":\n",
    "            return tf.nn.sigmoid(z)\n",
    "        else:\n",
    "            return z\n",
    "     \n",
    "\n",
    "def deconv_layer(X, filters, filter_size, stride, output_shape, name, collection, activation=None):\n",
    "    \"\"\"Create a new deconvolution layer with Xavier initializer\"\"\"\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   filters, in_channels),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "                \n",
    "        # add parameters to the collection\n",
    "        tf.add_to_collection(collection, W)\n",
    "        tf.add_to_collection(collection, b)\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d_transpose(X, W, output_shape=output_shape,\n",
    "                                      strides=(1, stride, stride, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "        \n",
    "def residual_layer(X, filter_size, stride, name, collection, activation=None):\n",
    "    \"\"\"Create a new residual convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    # get number of input filters     \n",
    "    filters = in_channels = int(X.get_shape()[3])\n",
    "    \n",
    "    # add residuals to convolution     \n",
    "    z = X + conv_layer(X, filters, filter_size, stride, name, collection)\n",
    "\n",
    "    # activation function\n",
    "    if activation == \"relu\":\n",
    "        return tf.nn.relu(z)\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "            \n",
    "    # load training data from input queues     \n",
    "    X = inputs(style_X, BATCH_SIZE, EPOCHS)\n",
    "    Y = inputs(style_Y, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # normalize the images     \n",
    "    X = (tf.cast(X, tf.float32) / 255.0)\n",
    "    Y = (tf.cast(Y, tf.float32) / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ G(X) -> Y ]==================#\n",
    "    \n",
    "    X_conv_1 = conv_layer(X, 32, (9, 9), 1, \"X_conv_1\", \"GEN\", \"relu\")\n",
    "    X_conv_2 = conv_layer(X_conv_1, 64, (3, 3), 2, \"X_conv_2\", \"GEN\", \"relu\")\n",
    "    X_conv_3 = conv_layer(X_conv_2, 128, (3, 3), 2, \"X_conv_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    X_res_1 = residual_layer(X_conv_3, (1, 1), 1, \"X_res_1\", \"GEN\", \"relu\")\n",
    "    X_res_2 = residual_layer(X_res_1, (1, 1), 1, \"X_res_2\", \"GEN\", \"relu\")\n",
    "    X_res_3 = residual_layer(X_res_2, (1, 1), 1, \"X_res_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    X_deconv_2 = deconv_layer(X_res_3, 64, (3, 3), 2, [BATCH_SIZE, 75, 75, 64], \"X_deconv_2\", \"GEN\", \"relu\")\n",
    "    X_deconv_1 = deconv_layer(X_deconv_2, 32, (3, 3), 2, [BATCH_SIZE, 150, 150, 32], \"X_deconv_1\", \"GEN\", \"relu\")\n",
    "    G_x = deconv_layer(X_deconv_1, 3, (9, 9), 1, [BATCH_SIZE, 150, 150, 3], \"G_x\", \"GEN\", \"relu\")\n",
    "    \n",
    "    #==================[ F(Y) -> X ]==================#\n",
    "    \n",
    "    Y_conv_1 = conv_layer(Y, 32, (9, 9), 1, \"Y_conv_1\", \"GEN\", \"relu\")\n",
    "    Y_conv_2 = conv_layer(Y_conv_1, 64, (3, 3), 2, \"Y_conv_2\", \"GEN\", \"relu\")\n",
    "    Y_conv_3 = conv_layer(Y_conv_2, 128, (3, 3), 2, \"Y_conv_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    Y_res_1 = residual_layer(Y_conv_3, (1, 1), 1, \"Y_res_1\", \"GEN\", \"relu\")\n",
    "    Y_res_2 = residual_layer(Y_res_1, (1, 1), 1, \"Y_res_2\", \"GEN\", \"relu\")\n",
    "    Y_res_3 = residual_layer(Y_res_2, (1, 1), 1, \"Y_res_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    Y_deconv_2 = deconv_layer(Y_res_3, 64, (3, 3), 2, [BATCH_SIZE, 75, 75, 64], \"Y_deconv_2\", \"GEN\", \"relu\")\n",
    "    Y_deconv_1 = deconv_layer(Y_deconv_2, 32, (3, 3), 2, [BATCH_SIZE, 150, 150, 32], \"Y_deconv_1\", \"GEN\", \"relu\") \n",
    "    F_y = deconv_layer(Y_deconv_1, 3, (9, 9), 1, [BATCH_SIZE, 150, 150, 3], \"F_y\", \"GEN\", \"relu\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Dy ]==================#\n",
    "    \n",
    "    Dy_input = tf.concat([G_x, Y], 0)\n",
    "    Dy_labels = tf.constant(BATCH_SIZE * [0] + BATCH_SIZE * [1], dtype=tf.float32)\n",
    "    \n",
    "    Dy_conv_1 = conv_layer(Dy_input, 32, (3, 3), 1, \"Dy_conv_1\", \"DSC\", \"relu\")\n",
    "    Dy_conv_2 = conv_layer(Dy_conv_1, 32, (3, 3), 1, \"Dy_conv_2\", \"DSC\", \"relu\")\n",
    "    Dy_conv_3 = conv_layer(Dy_conv_2, 1, (5, 5), 5, \"Dy_conv_3\", \"DSC\", \"sigmoid\")\n",
    "    Dy = tf.reduce_mean(tf.contrib.layers.flatten(Dy_conv_3), -1, keep_dims=False, name=\"Dy\")\n",
    "    \n",
    "    #==================[ Dx ]==================#\n",
    "    \n",
    "    Dx_input = tf.concat([F_y, X], 0)\n",
    "    Dx_labels = tf.constant(BATCH_SIZE * [0] + BATCH_SIZE * [1], dtype=tf.float32)\n",
    "    \n",
    "    Dx_conv_1 = conv_layer(Dx_input, 32, (3, 3), 1, \"Dx_conv_1\", \"DSC\", \"relu\")\n",
    "    Dx_conv_2 = conv_layer(Dx_conv_1, 32, (3, 3), 1, \"Dx_conv_2\", \"DSC\", \"relu\")\n",
    "    Dx_conv_3 = conv_layer(Dx_conv_2, 1, (5, 5), 5, \"Dx_conv_3\", \"DSC\", \"sigmoid\")\n",
    "    Dx = tf.reduce_mean(tf.contrib.layers.flatten(Dx_conv_3), -1, keep_dims=False, name=\"Dx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Adversarial Loss ]==================#\n",
    "    \n",
    "    L_GAN_X = - tf.nn.sigmoid_cross_entropy_with_logits(labels=Dy_labels,\n",
    "                                                        logits=Dy)\n",
    "    L_GAN_Y = - tf.nn.sigmoid_cross_entropy_with_logits(labels=Dx_labels,\n",
    "                                                        logits=Dx)\n",
    "    \n",
    "    #==================[ Consistency Loss ]==================#\n",
    "    \n",
    "    L_CYC = tf.constant(0.0)\n",
    "    \n",
    "    #=====================[ Final Loss ]=====================#\n",
    "    \n",
    "    Loss = tf.reduce_mean(L_GAN_X) + tf.reduce_mean(L_GAN_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "    generators_op = optimizer.minimize(Loss, var_list=tf.get_collection(\"GEN\"))\n",
    "    discriminators_op = optimizer.minimize(-Loss, var_list=tf.get_collection(\"DSC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     1 | Generator         0 | loss = -1.447\n",
      "Step     1 | Generator         1 | loss = -1.767\n",
      "Step     1 | Generator         2 | loss = -1.774\n",
      "Step     1 | Generator         3 | loss = -1.780\n",
      "Step     1 | Generator         4 | loss = -1.780\n",
      "Step     1 | Generator         5 | loss = -1.780\n",
      "Step     1 | Generator         6 | loss = -1.780\n",
      "Step     1 | Generator         7 | loss = -1.780\n",
      "Step     1 | Generator         8 | loss = -1.780\n",
      "Step     1 | Generator         9 | loss = -1.780\n",
      "Step     1 | Generator        10 | loss = -1.780\n",
      "Step     1 | Generator        11 | loss = -1.780\n",
      "Step     1 | Generator        12 | loss = -1.780\n",
      "Step     1 | Generator        13 | loss = -1.780\n",
      "Step     1 | Generator        14 | loss = -1.780\n",
      "Step     1 | Generator        15 | loss = -1.780\n",
      "Step     1 | Generator        16 | loss = -1.780\n",
      "Step     1 | Generator        17 | loss = -1.780\n",
      "Step     1 | Generator        18 | loss = -1.780\n",
      "Step     1 | Generator        19 | loss = -1.780\n",
      "Step     1 | Generator        20 | loss = -1.780\n",
      "Step     1 | Generator        21 | loss = -1.780\n",
      "Step     1 | Generator        22 | loss = -1.780\n",
      "Step     1 | Generator        23 | loss = -1.780\n",
      "Step     1 | Generator        24 | loss = -1.780\n",
      "Step     1 | Discriminator     0 | loss = -1.780\n",
      "Step     1 | Discriminator     1 | loss = -1.662\n",
      "Step     1 | Discriminator     2 | loss = -1.629\n",
      "Step     1 | Discriminator     3 | loss = -1.627\n",
      "Step     1 | Discriminator     4 | loss = -1.627\n",
      "Step     1 | Discriminator     5 | loss = -1.627\n",
      "Step     1 | Discriminator     6 | loss = -1.627\n",
      "Step     1 | Discriminator     7 | loss = -1.627\n",
      "Step     1 | Discriminator     8 | loss = -1.627\n",
      "Step     1 | Discriminator     9 | loss = -1.627\n",
      "Step     1 | Discriminator    10 | loss = -1.627\n",
      "Step     1 | Discriminator    11 | loss = -1.627\n",
      "Step     1 | Discriminator    12 | loss = -1.627\n",
      "Step     1 | Discriminator    13 | loss = -1.627\n",
      "Step     1 | Discriminator    14 | loss = -1.627\n",
      "Step     1 | Discriminator    15 | loss = -1.627\n",
      "Step     1 | Discriminator    16 | loss = -1.627\n",
      "Step     1 | Discriminator    17 | loss = -1.627\n",
      "Step     1 | Discriminator    18 | loss = -1.627\n",
      "Step     1 | Discriminator    19 | loss = -1.627\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a9984aaa6ab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# train the discriminators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDSC_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdiscriminators_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             print(\"Step {0:5d} | Discriminator {1:5d} | loss = {2:6.3f}\".format(\n\u001b[0;32m     18\u001b[0m                     step, i, loss))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0   \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "        step += 1\n",
    "        \n",
    "        # train the generators\n",
    "        for i in range(GEN_STEPS):\n",
    "            _, loss = sess.run([generators_op, Loss])\n",
    "            print(\"Step {0:5d} | Generator     {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))\n",
    "            \n",
    "            \n",
    "        # train the discriminators\n",
    "        for i in range(DSC_STEPS):\n",
    "            _, loss = sess.run([discriminators_op, Loss])\n",
    "            print(\"Step {0:5d} | Discriminator {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         x, g_x, y, f_y, dy, dy_labels = sess.run([X, G_x, Y, F_y, Dy, Dy_labels])\n",
    "#         print(dy)\n",
    "#         print(dy_labels)\n",
    "#         fig=plt.figure(figsize=(8, 8))\n",
    "#         fig.add_subplot(2, 2, 1)\n",
    "#         plt.imshow(x[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 2)\n",
    "#         plt.imshow(g_x[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 3)\n",
    "#         plt.imshow(y[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 4)\n",
    "#         plt.imshow(f_y[0, :, :, :3])\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
