{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Styles for transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {\n",
    "    0 : 'Gorodets',\n",
    "    1 : 'Gzhel',\n",
    "    2 : 'Iznik',\n",
    "    3 : 'Khokhloma',\n",
    "    4 : 'Neglyubka',\n",
    "    5 : 'Wycinanki_Å‚owickie',\n",
    "    6 : 'Wzory_kaszubskie'\n",
    "}\n",
    "\n",
    "style_X = styles[1]\n",
    "style_Y = styles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', style_X + ' =|= ' + style_Y, restoring_name)\n",
    "saving_path = os.path.join('models', style_X + ' =|= ' + style_Y, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 150\n",
    "\n",
    "LAMBDA = 0\n",
    "GEN_STEPS = 30\n",
    "DSC_STEPS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, stride, name, collection, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # add parameters to the collection\n",
    "        tf.add_to_collection(collection, W)\n",
    "        tf.add_to_collection(collection, b)\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, stride, stride, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        elif activation == \"sigmoid\":\n",
    "            return tf.nn.sigmoid(z)\n",
    "        else:\n",
    "            return z\n",
    "     \n",
    "\n",
    "def deconv_layer(X, filters, filter_size, stride, output_shape, name, collection, activation=None):\n",
    "    \"\"\"Create a new deconvolution layer with Xavier initializer\"\"\"\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "\n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   filters, in_channels),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "                \n",
    "        # add parameters to the collection\n",
    "        tf.add_to_collection(collection, W)\n",
    "        tf.add_to_collection(collection, b)\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d_transpose(X, W, output_shape=output_shape,\n",
    "                                      strides=(1, stride, stride, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "\n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "        \n",
    "def residual_layer(X, filter_size, stride, name, collection, activation=None):\n",
    "    \"\"\"Create a new residual convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    # get number of input filters     \n",
    "    filters = in_channels = int(X.get_shape()[3])\n",
    "    \n",
    "    # add residuals to convolution     \n",
    "    z = X + conv_layer(X, filters, filter_size, stride, name, collection)\n",
    "\n",
    "    # activation function\n",
    "    if activation == \"relu\":\n",
    "        return tf.nn.relu(z)\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "            \n",
    "    # load training data from input queues     \n",
    "    X = inputs(style_X, BATCH_SIZE, EPOCHS)\n",
    "    Y = inputs(style_Y, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # normalize the images     \n",
    "    X = (tf.cast(X, tf.float32) / 255.0)\n",
    "    Y = (tf.cast(Y, tf.float32) / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ G(X) -> Y ]==================#\n",
    "    \n",
    "    X_conv_1 = conv_layer(X, 32, (9, 9), 1, \"X_conv_1\", \"GEN\", \"relu\")\n",
    "    X_conv_2 = conv_layer(X_conv_1, 64, (3, 3), 2, \"X_conv_2\", \"GEN\", \"relu\")\n",
    "    X_conv_3 = conv_layer(X_conv_2, 128, (3, 3), 2, \"X_conv_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    X_res_1 = residual_layer(X_conv_3, (1, 1), 1, \"X_res_1\", \"GEN\", \"relu\")\n",
    "    X_res_2 = residual_layer(X_res_1, (1, 1), 1, \"X_res_2\", \"GEN\", \"relu\")\n",
    "    X_res_3 = residual_layer(X_res_2, (1, 1), 1, \"X_res_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    X_deconv_2 = deconv_layer(X_res_3, 64, (3, 3), 2, [BATCH_SIZE, 75, 75, 64], \"X_deconv_2\", \"GEN\", \"relu\")\n",
    "    X_deconv_1 = deconv_layer(X_deconv_2, 32, (3, 3), 2, [BATCH_SIZE, 150, 150, 32], \"X_deconv_1\", \"GEN\", \"relu\")\n",
    "    G_x = deconv_layer(X_deconv_1, 3, (9, 9), 1, [BATCH_SIZE, 150, 150, 3], \"G_x\", \"GEN\", \"relu\")\n",
    "    \n",
    "    #==================[ F(Y) -> X ]==================#\n",
    "    \n",
    "    Y_conv_1 = conv_layer(Y, 32, (9, 9), 1, \"Y_conv_1\", \"GEN\", \"relu\")\n",
    "    Y_conv_2 = conv_layer(Y_conv_1, 64, (3, 3), 2, \"Y_conv_2\", \"GEN\", \"relu\")\n",
    "    Y_conv_3 = conv_layer(Y_conv_2, 128, (3, 3), 2, \"Y_conv_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    Y_res_1 = residual_layer(Y_conv_3, (1, 1), 1, \"Y_res_1\", \"GEN\", \"relu\")\n",
    "    Y_res_2 = residual_layer(Y_res_1, (1, 1), 1, \"Y_res_2\", \"GEN\", \"relu\")\n",
    "    Y_res_3 = residual_layer(Y_res_2, (1, 1), 1, \"Y_res_3\", \"GEN\", \"relu\")\n",
    "    \n",
    "    Y_deconv_2 = deconv_layer(Y_res_3, 64, (3, 3), 2, [BATCH_SIZE, 75, 75, 64], \"Y_deconv_2\", \"GEN\", \"relu\")\n",
    "    Y_deconv_1 = deconv_layer(Y_deconv_2, 32, (3, 3), 2, [BATCH_SIZE, 150, 150, 32], \"Y_deconv_1\", \"GEN\", \"relu\") \n",
    "    F_y = deconv_layer(Y_deconv_1, 3, (9, 9), 1, [BATCH_SIZE, 150, 150, 3], \"F_y\", \"GEN\", \"relu\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Dy ]==================#\n",
    "    \n",
    "    Dy_input = tf.concat([G_x, Y], 0)\n",
    "    Dy_labels = tf.constant(BATCH_SIZE * [0] + BATCH_SIZE * [1], dtype=tf.float32)\n",
    "    \n",
    "    Dy_conv_1 = conv_layer(Dy_input, 32, (3, 3), 1, \"Dy_conv_1\", \"DSC\", \"relu\")\n",
    "    Dy_conv_2 = conv_layer(Dy_conv_1, 32, (3, 3), 1, \"Dy_conv_2\", \"DSC\", \"relu\")\n",
    "    Dy_conv_3 = conv_layer(Dy_conv_2, 1, (5, 5), 5, \"Dy_conv_3\", \"DSC\", \"sigmoid\")\n",
    "    Dy = tf.reduce_mean(tf.contrib.layers.flatten(Dy_conv_3), -1, keep_dims=False, name=\"Dy\")\n",
    "    \n",
    "    #==================[ Dx ]==================#\n",
    "    \n",
    "    Dx_input = tf.concat([F_y, X], 0)\n",
    "    Dx_labels = tf.constant(BATCH_SIZE * [0] + BATCH_SIZE * [1], dtype=tf.float32)\n",
    "    \n",
    "    Dx_conv_1 = conv_layer(Dx_input, 32, (3, 3), 1, \"Dx_conv_1\", \"DSC\", \"relu\")\n",
    "    Dx_conv_2 = conv_layer(Dx_conv_1, 32, (3, 3), 1, \"Dx_conv_2\", \"DSC\", \"relu\")\n",
    "    Dx_conv_3 = conv_layer(Dx_conv_2, 1, (5, 5), 5, \"Dx_conv_3\", \"DSC\", \"sigmoid\")\n",
    "    Dx = tf.reduce_mean(tf.contrib.layers.flatten(Dx_conv_3), -1, keep_dims=False, name=\"Dx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    #==================[ Adversarial Loss ]==================#\n",
    "    \n",
    "    L_GAN_X = - tf.nn.sigmoid_cross_entropy_with_logits(labels=Dy_labels,\n",
    "                                                        logits=Dy)\n",
    "    L_GAN_Y = - tf.nn.sigmoid_cross_entropy_with_logits(labels=Dx_labels,\n",
    "                                                        logits=Dx)\n",
    "    \n",
    "    #==================[ Consistency Loss ]==================#\n",
    "    \n",
    "    L_CYC = tf.constant(0.0)\n",
    "    \n",
    "    #=====================[ Final Loss ]=====================#\n",
    "    \n",
    "    Loss = tf.reduce_mean(L_GAN_X) + tf.reduce_mean(L_GAN_Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "    generators_op = optimizer.minimize(Loss, var_list=tf.get_collection(\"GEN\"))\n",
    "    discriminators_op = optimizer.minimize(-Loss, var_list=tf.get_collection(\"DSC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     1 | Generator         0 | loss = -1.478\n",
      "Step     1 | Generator         1 | loss = -1.478\n",
      "Step     1 | Generator         2 | loss = -1.479\n",
      "Step     1 | Generator         3 | loss = -1.481\n",
      "Step     1 | Generator         4 | loss = -1.483\n",
      "Step     1 | Generator         5 | loss = -1.495\n",
      "Step     1 | Generator         6 | loss = -1.512\n",
      "Step     1 | Generator         7 | loss = -1.545\n",
      "Step     1 | Generator         8 | loss = -1.581\n",
      "Step     1 | Generator         9 | loss = -1.612\n",
      "Step     1 | Generator        10 | loss = -1.634\n",
      "Step     1 | Generator        11 | loss = -1.638\n",
      "Step     1 | Generator        12 | loss = -1.637\n",
      "Step     1 | Generator        13 | loss = -1.640\n",
      "Step     1 | Generator        14 | loss = -1.643\n",
      "Step     1 | Generator        15 | loss = -1.642\n",
      "Step     1 | Generator        16 | loss = -1.644\n",
      "Step     1 | Generator        17 | loss = -1.645\n",
      "\n",
      "Done training -- epoch limit reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0   \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "        step += 1\n",
    "        \n",
    "        # train the generators\n",
    "        for i in range(GEN_STEPS):\n",
    "            _, loss = sess.run([generators_op, Loss])\n",
    "            print(\"Step {0:5d} | Generator     {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))\n",
    "            \n",
    "            \n",
    "        # train the discriminators\n",
    "        for i in range(DSC_STEPS):\n",
    "            _, loss = sess.run([discriminators_op, Loss])\n",
    "            print(\"Step {0:5d} | Discriminator {1:5d} | loss = {2:6.3f}\".format(\n",
    "                    step, i, loss))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         x, g_x, y, f_y, dy, dy_labels = sess.run([X, G_x, Y, F_y, Dy, Dy_labels])\n",
    "#         print(dy)\n",
    "#         print(dy_labels)\n",
    "#         fig=plt.figure(figsize=(8, 8))\n",
    "#         fig.add_subplot(2, 2, 1)\n",
    "#         plt.imshow(x[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 2)\n",
    "#         plt.imshow(g_x[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 3)\n",
    "#         plt.imshow(y[0, :, :, :3])\n",
    "#         fig.add_subplot(2, 2, 4)\n",
    "#         plt.imshow(f_y[0, :, :, :3])\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
