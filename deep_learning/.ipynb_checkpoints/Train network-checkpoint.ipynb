{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "\n",
    "classes_dict = {\n",
    "    'by_country' : 4,\n",
    "    'by_style' : 7,\n",
    "    'by_product' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = 'by_style'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'by_notebook.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', problem_type, restoring_name)\n",
    "saving_path = os.path.join('models', problem_type, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1e6\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_BATCH = 32\n",
    "\n",
    "IMG_SIZE = 150\n",
    "CLASSES = classes_dict[problem_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, name, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "        \n",
    "def pooling_layer(X, kernel_size):\n",
    "    \"\"\"Perform max pooling\"\"\"\n",
    "    \n",
    "    return tf.nn.max_pool(X,\n",
    "                          ksize=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          strides=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Create a new fully connected layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # create the parameter structures     \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(n_inputs, n_neurons),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(n_neurons),\n",
    "                            name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "    \n",
    "    # decide the dataset input type\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "    # load training data from input queues     \n",
    "    images_trn, labels_trn = inputs(problem_type, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # load validation data from feed dictionary\n",
    "    images_val = tf.placeholder(tf.uint8, shape=[VALIDATION_BATCH, IMG_SIZE, IMG_SIZE, 3])\n",
    "    labels_val = tf.placeholder(tf.int32, shape=[VALIDATION_BATCH,])\n",
    "    \n",
    "    # choose the input\n",
    "    images, labels = tf.cond(is_training, lambda: (images_trn, labels_trn),\n",
    "                                          lambda: (images_val, labels_val))\n",
    "\n",
    "    # normalize the images     \n",
    "    images = (tf.cast(images, tf.float32) / 255.0)\n",
    "    # correct the labels     \n",
    "    labels = labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Apps\\Anaconda2\\envs\\work\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    #==================[ CONVOLUTIONAL LAYERS ]==================#\n",
    "    \n",
    "    images_conv_11 = conv_layer(images, 16, (5, 5), \"conv_11\", \"relu\")\n",
    "    images_conv_12 = conv_layer(images_conv_11, 16, (5, 5), \"conv_12\", \"relu\")\n",
    "    images_pool_1  = pooling_layer(images_conv_12, (3, 3))\n",
    "    \n",
    "    images_conv_21 = conv_layer(images_pool_1, 32, (3, 3), \"conv_21\", \"relu\")\n",
    "    images_conv_22 = conv_layer(images_conv_21, 32, (3, 3), \"conv_22\", \"relu\")\n",
    "    images_pool_2  = pooling_layer(images_conv_22, (3, 3))\n",
    "        \n",
    "    #==================[     DENSE LAYERS     ]==================#\n",
    "    \n",
    "    images_flatten = tf.contrib.layers.flatten(images_pool_2)\n",
    "    images_dense_1 = dense_layer(images_flatten, 128, \"dense_1\", \"relu\")\n",
    "    images_dense_2 = dense_layer(images_dense_1, 128, \"dense_2\", \"relu\")\n",
    "    \n",
    "    #==================[     OUTPUT LAYER     ]==================#\n",
    "    \n",
    "    logits = dense_layer(images_dense_2, CLASSES, \"logits\")\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "    #==================[     OPTIMIZATION     ]==================#\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    #==================[      EVALUATION      ]==================#\n",
    "    \n",
    "    _, accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions=tf.argmax(logits, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorboard old data\n",
    "try:\n",
    "    shutil.rmtree(os.path.join('tensorboard', problem_type))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    summary_conv_12 = tf.summary.image('conv_12', images_conv_12[:,:,:,:3])\n",
    "    summary_conv_22 = tf.summary.image('conv_22', images_conv_22[:,:,:,:3])\n",
    "    \n",
    "    train_loss = tf.summary.scalar('training_loss', loss)\n",
    "    val_loss = tf.summary.scalar('validation_loss', loss)\n",
    "    train_acc = tf.summary.scalar('training_accuracy', accuracy)\n",
    "    val_acc = tf.summary.scalar('validation_accuracy', accuracy)\n",
    "\n",
    "    convs_merged = tf.summary.merge([summary_conv_12, summary_conv_22])\n",
    "    train_merged = tf.summary.merge([train_loss, train_acc])\n",
    "    val_merged = tf.summary.merge([val_loss, val_acc])\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join('tensorboard', problem_type), sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "images_validation = np.load(os.path.join('data_' + problem_type, 'testing_data.dat'))\n",
    "labels_validation = np.load(os.path.join('data_' + problem_type, 'testing_labels.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    25 : Training >> loss = 1.955 accuracy = 16.34%  ||  Testing >> loss = 1.901 accuracy = 25.04%\n",
      "Step    50 : Training >> loss = 1.757 accuracy = 21.40%  ||  Testing >> loss = 2.031 accuracy = 24.06%\n",
      "Step    75 : Training >> loss = 1.498 accuracy = 20.50%  ||  Testing >> loss = 3.294 accuracy = 18.92%\n",
      "Step   100 : Training >> loss = 1.295 accuracy = 17.76%  ||  Testing >> loss = 4.647 accuracy = 17.04%\n",
      "Step   125 : Training >> loss = 1.167 accuracy = 17.01%  ||  Testing >> loss = 4.684 accuracy = 16.86%\n",
      "Step   150 : Training >> loss = 1.015 accuracy = 17.28%  ||  Testing >> loss = 4.779 accuracy = 16.90%\n",
      "Step   175 : Training >> loss = 1.048 accuracy = 17.20%  ||  Testing >> loss = 4.667 accuracy = 16.80%\n",
      "Step   200 : Training >> loss = 0.977 accuracy = 16.90%  ||  Testing >> loss = 5.868 accuracy = 16.82%\n",
      "Step   225 : Training >> loss = 1.014 accuracy = 17.04%  ||  Testing >> loss = 5.587 accuracy = 17.00%\n",
      "Step   250 : Training >> loss = 1.079 accuracy = 17.30%  ||  Testing >> loss = 6.279 accuracy = 17.21%\n",
      "Step   275 : Training >> loss = 0.867 accuracy = 17.40%  ||  Testing >> loss = 6.691 accuracy = 17.39%\n",
      "Step   300 : Training >> loss = 0.825 accuracy = 17.59%  ||  Testing >> loss = 6.208 accuracy = 17.56%\n",
      "Step   325 : Training >> loss = 0.911 accuracy = 17.83%  ||  Testing >> loss = 4.661 accuracy = 17.80%\n",
      "Step   350 : Training >> loss = 0.876 accuracy = 18.20%  ||  Testing >> loss = 6.752 accuracy = 18.16%\n",
      "Step   375 : Training >> loss = 0.863 accuracy = 18.37%  ||  Testing >> loss = 6.798 accuracy = 18.32%\n",
      "Step   400 : Training >> loss = 0.820 accuracy = 18.58%  ||  Testing >> loss = 6.337 accuracy = 18.52%\n",
      "Step   425 : Training >> loss = 0.866 accuracy = 18.63%  ||  Testing >> loss = 7.432 accuracy = 18.55%\n",
      "Step   450 : Training >> loss = 0.804 accuracy = 18.78%  ||  Testing >> loss = 7.749 accuracy = 18.70%\n",
      "Step   475 : Training >> loss = 0.748 accuracy = 18.86%  ||  Testing >> loss = 8.476 accuracy = 18.79%\n",
      "Step   500 : Training >> loss = 0.705 accuracy = 18.89%  ||  Testing >> loss = 9.511 accuracy = 18.83%\n",
      "Step   525 : Training >> loss = 0.771 accuracy = 19.06%  ||  Testing >> loss = 9.254 accuracy = 18.99%\n",
      "Step   550 : Training >> loss = 0.613 accuracy = 19.22%  ||  Testing >> loss = 8.009 accuracy = 19.22%\n",
      "Step   575 : Training >> loss = 0.569 accuracy = 19.46%  ||  Testing >> loss = 9.692 accuracy = 19.41%\n",
      "Step   600 : Training >> loss = 0.629 accuracy = 19.60%  ||  Testing >> loss = 10.401 accuracy = 19.60%\n",
      "Step   625 : Training >> loss = 0.508 accuracy = 19.75%  ||  Testing >> loss = 12.010 accuracy = 19.74%\n",
      "Step   650 : Training >> loss = 0.637 accuracy = 19.92%  ||  Testing >> loss = 10.213 accuracy = 19.88%\n",
      "Step   675 : Training >> loss = 0.463 accuracy = 20.07%  ||  Testing >> loss = 13.286 accuracy = 20.04%\n",
      "Step   700 : Training >> loss = 0.616 accuracy = 20.18%  ||  Testing >> loss = 9.726 accuracy = 20.14%\n",
      "Step   725 : Training >> loss = 0.685 accuracy = 20.36%  ||  Testing >> loss = 8.242 accuracy = 20.32%\n",
      "Step   750 : Training >> loss = 0.526 accuracy = 20.42%  ||  Testing >> loss = 10.908 accuracy = 20.37%\n",
      "Step   775 : Training >> loss = 0.540 accuracy = 20.55%  ||  Testing >> loss = 10.195 accuracy = 20.51%\n",
      "Step   800 : Training >> loss = 0.559 accuracy = 20.71%  ||  Testing >> loss = 12.221 accuracy = 20.65%\n",
      "Step   825 : Training >> loss = 0.684 accuracy = 20.81%  ||  Testing >> loss = 10.493 accuracy = 20.77%\n",
      "Step   850 : Training >> loss = 0.472 accuracy = 20.97%  ||  Testing >> loss = 11.928 accuracy = 20.98%\n",
      "Step   875 : Training >> loss = 0.440 accuracy = 21.16%  ||  Testing >> loss = 12.933 accuracy = 21.14%\n",
      "Step   900 : Training >> loss = 0.473 accuracy = 21.24%  ||  Testing >> loss = 12.242 accuracy = 21.24%\n",
      "Step   925 : Training >> loss = 0.430 accuracy = 21.40%  ||  Testing >> loss = 12.445 accuracy = 21.35%\n",
      "Step   950 : Training >> loss = 0.407 accuracy = 21.51%  ||  Testing >> loss = 15.438 accuracy = 21.46%\n",
      "Step   975 : Training >> loss = 0.374 accuracy = 21.62%  ||  Testing >> loss = 15.455 accuracy = 21.58%\n",
      "Step  1000 : Training >> loss = 0.447 accuracy = 21.76%  ||  Testing >> loss = 13.532 accuracy = 21.75%\n",
      "Step  1025 : Training >> loss = 0.435 accuracy = 21.90%  ||  Testing >> loss = 13.440 accuracy = 21.88%\n",
      "Step  1050 : Training >> loss = 0.360 accuracy = 22.06%  ||  Testing >> loss = 14.645 accuracy = 22.05%\n",
      "Step  1075 : Training >> loss = 0.578 accuracy = 22.20%  ||  Testing >> loss = 10.265 accuracy = 22.16%\n",
      "Step  1100 : Training >> loss = 0.366 accuracy = 22.26%  ||  Testing >> loss = 16.883 accuracy = 22.23%\n",
      "Step  1125 : Training >> loss = 0.358 accuracy = 22.37%  ||  Testing >> loss = 15.192 accuracy = 22.36%\n",
      "Step  1150 : Training >> loss = 0.321 accuracy = 22.54%  ||  Testing >> loss = 14.869 accuracy = 22.54%\n",
      "Step  1175 : Training >> loss = 0.324 accuracy = 22.71%  ||  Testing >> loss = 16.866 accuracy = 22.67%\n",
      "Step  1200 : Training >> loss = 0.286 accuracy = 22.79%  ||  Testing >> loss = 14.450 accuracy = 22.78%\n",
      "Step  1225 : Training >> loss = 0.219 accuracy = 22.98%  ||  Testing >> loss = 15.431 accuracy = 22.96%\n",
      "Step  1250 : Training >> loss = 0.236 accuracy = 23.13%  ||  Testing >> loss = 15.698 accuracy = 23.11%\n",
      "Step  1275 : Training >> loss = 0.247 accuracy = 23.24%  ||  Testing >> loss = 16.646 accuracy = 23.22%\n",
      "Step  1300 : Training >> loss = 0.245 accuracy = 23.35%  ||  Testing >> loss = 17.073 accuracy = 23.32%\n",
      "Done training -- epoch limit reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0\n",
    "    MILESTONE = 100\n",
    "    \n",
    "    min_val_loss = math.inf\n",
    "    max_val_acc = -math.inf\n",
    "    \n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    samples_count = 0\n",
    "    \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        _, loss_value, acc_value, summary, img_sum = sess.run(\n",
    "                [training_op, loss, accuracy, train_merged, convs_merged], feed_dict={\n",
    "            is_training : True,\n",
    "            images_val : images_validation[:VALIDATION_BATCH],\n",
    "            labels_val : labels_validation[:VALIDATION_BATCH]\n",
    "        })\n",
    "        \n",
    "        # save stats to log         \n",
    "        summary_writer.add_summary(summary, step)\n",
    "        summary_writer.add_summary(img_sum, step)\n",
    "        \n",
    "        acc_sum += acc_value\n",
    "        loss_sum += loss_value\n",
    "        samples_count += 1\n",
    "                        \n",
    "        if step % MILESTONE == 0:\n",
    "            \n",
    "            #===================[ TRAINING SCORE ]===================#\n",
    "            \n",
    "            training_acc = acc_sum / samples_count\n",
    "            training_loss = loss_sum / samples_count\n",
    "                \n",
    "            print(\"\\nStep {0:5d} : Training >> loss = {1:.3f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, training_loss, 100 * training_acc), flush=True, end=\"\")\n",
    "            \n",
    "            acc_sum = 0\n",
    "            loss_sum = 0\n",
    "            samples_count = 0  \n",
    "        \n",
    "            #===================[ TESTING SCORE ]===================#\n",
    "            \n",
    "            val_acc_sum = 0\n",
    "            val_loss_sum = 0\n",
    "            val_samples_count = 0  \n",
    "            \n",
    "            confussion_matrix = np.empty((CLASSES, CLASSES), dtype=np.int32)\n",
    "            \n",
    "            for i in range(VALIDATION_BATCH, len(labels_validation), VALIDATION_BATCH):\n",
    "                \n",
    "                loss_value, acc_value, summary = sess.run(\n",
    "                    [loss, accuracy, val_merged], feed_dict={\n",
    "                    is_training : False,\n",
    "                    images_val : images_validation[(i - VALIDATION_BATCH):i],\n",
    "                    labels_val : labels_validation[(i - VALIDATION_BATCH):i]\n",
    "                })\n",
    "                \n",
    "                val_acc_sum += acc_value\n",
    "                val_loss_sum += loss_value\n",
    "                val_samples_count += 1\n",
    "                \n",
    "            # save stats to log         \n",
    "            summary_writer.add_summary(summary, step)\n",
    "                \n",
    "            validation_acc = val_acc_sum / val_samples_count\n",
    "            validation_loss = val_loss_sum / val_samples_count\n",
    "            \n",
    "            print(\"  ||  Testing >> loss = {1:.3f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, validation_loss, 100 * validation_acc), flush=True, end=\"\")\n",
    "                        \n",
    "            saving_condition = (validation_loss < min_val_loss)\n",
    "\n",
    "            # save the model for later use         \n",
    "            if saving_mode and saving_condition:\n",
    "                saver.save(sess, saving_path)\n",
    "                \n",
    "            min_val_loss = min(min_val_loss, validation_loss)\n",
    "            max_val_acc  = max(max_val_acc,  validation_acc)\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
