{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "\n",
    "classes_dict = {\n",
    "    'by_country' : 4,\n",
    "    'by_style' : 7,\n",
    "    'by_product' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = 'by_style' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'model.ckpt'\n",
    "saving_name = 'model.ckpt'\n",
    "\n",
    "\n",
    "restoring_path = os.path.join('models', problem_type, restoring_name)\n",
    "saving_path = os.path.join('models', problem_type, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 8\n",
    "VALIDATION_BATCH = 16\n",
    "\n",
    "IMG_SIZE = 150\n",
    "CLASSES = classes_dict[problem_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, name, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "        \n",
    "def pooling_layer(X, kernel_size):\n",
    "    \"\"\"Perform max pooling\"\"\"\n",
    "    \n",
    "    return tf.nn.max_pool(X,\n",
    "                          ksize=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          strides=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Create a new fully connected layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # create the parameter structures     \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(n_inputs, n_neurons),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(n_neurons),\n",
    "                            name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "    \n",
    "    # decide the dataset input type\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    \n",
    "    # load training data from input queues     \n",
    "    images_trn, labels_trn = inputs(problem_type, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # load validation data from feed dictionary\n",
    "    images_val = tf.placeholder(tf.uint8, shape=[VALIDATION_BATCH, IMG_SIZE, IMG_SIZE, 3])\n",
    "    labels_val = tf.placeholder(tf.int32, shape=[VALIDATION_BATCH,])\n",
    "    \n",
    "    # choose the input\n",
    "    images = tf.cond(is_training, lambda: images_trn, lambda: images_val)\n",
    "    labels = tf.cond(is_training, lambda: labels_trn, lambda: labels_val)\n",
    "\n",
    "    # normalize the images     \n",
    "    images = (tf.cast(images, tf.float32) / 255.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    #==================[ CONVOLUTIONAL LAYERS ]==================#\n",
    "    \n",
    "    images_conv_11 = conv_layer(images, 32, (5, 5), \"conv_11\", \"relu\")\n",
    "    images_conv_12 = conv_layer(images_conv_11, 32, (5, 5), \"conv_12\", \"relu\")\n",
    "    images_pool_1  = pooling_layer(images_conv_12, (2, 2))\n",
    "    \n",
    "    images_conv_21 = conv_layer(images_pool_1, 64, (3, 3), \"conv_21\", \"relu\")\n",
    "    images_conv_22 = conv_layer(images_conv_21, 64, (3, 3), \"conv_22\", \"relu\")\n",
    "    images_pool_2  = pooling_layer(images_conv_22, (2, 2))\n",
    "    \n",
    "    images_conv_31 = conv_layer(images_pool_2, 64, (3, 3), \"conv_31\", \"relu\")\n",
    "    images_conv_32 = conv_layer(images_conv_31, 64, (3, 3), \"conv_32\", \"relu\")\n",
    "    images_pool_3  = pooling_layer(images_conv_32, (2, 2))\n",
    "    \n",
    "    #==================[     DENSE LAYERS     ]==================#\n",
    "    \n",
    "    images_flatten = tf.contrib.layers.flatten(images_pool_3)\n",
    "    images_dense_1 = dense_layer(images_flatten, 256, \"dense_1\", \"relu\")\n",
    "    images_dense_2 = dense_layer(images_dense_1, 256, \"dense_2\", \"relu\")\n",
    "    \n",
    "    #==================[     OUTPUT LAYER     ]==================#\n",
    "    \n",
    "    logits = dense_layer(images_dense_2, CLASSES, \"logits\")\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "    #==================[     OPTIMIZATION     ]==================#\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    tf.summary.image('conv_12', images_conv_12[:,:,:,:3])\n",
    "    tf.summary.image('conv_22', images_conv_22[:,:,:,:3])\n",
    "    tf.summary.image('conv_32', images_conv_32[:,:,:,:3])\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\n",
    "        os.path.join('tensorboard', problem_type), sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "images_validation = np.load(os.path.join('data_' + problem_type, 'testing_data.dat'))\n",
    "labels_validation = np.load(os.path.join('data_' + problem_type, 'testing_labels.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 : loss = 0.74\n",
      "Step 20 : loss = 0.03\n",
      "Step 30 : loss = 0.94\n",
      "Step 40 : loss = 0.55\n",
      "Step 50 : loss = 0.54\n",
      "Step 60 : loss = 0.59\n",
      "Step 70 : loss = 0.62\n",
      "Step 80 : loss = 0.62\n",
      "Step 90 : loss = 0.33\n",
      "Step 100 : loss = 0.91\n",
      "Step 110 : loss = 0.75\n",
      "Step 120 : loss = 0.80\n",
      "Step 130 : loss = 1.04\n",
      "Step 140 : loss = 0.75\n",
      "Step 150 : loss = 1.50\n",
      "Step 160 : loss = 1.46\n",
      "Step 170 : loss = 1.11\n",
      "Step 180 : loss = 0.98\n",
      "Step 190 : loss = 0.77\n",
      "Step 200 : loss = 1.04\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0\n",
    "    \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        _, loss_value, summary = sess.run([training_op, loss, merged], feed_dict={\n",
    "            is_training : True,\n",
    "            images_val : images_validation[:16],\n",
    "            labels_val : labels_validation[:16]\n",
    "        })\n",
    "        \n",
    "        # display status once in a while         \n",
    "        if step % 10 == 0:\n",
    "            print(\"Step {0} : loss = {1:.2f}\".format(step, loss_value), flush=True)\n",
    "            train_writer.add_summary(summary, step)\n",
    "        \n",
    "        saving_condition = True\n",
    "        \n",
    "        # save the model for later use         \n",
    "        if saving_mode and saving_condition:\n",
    "            saver.save(sess, saving_path)\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
