{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "\n",
    "classes_dict = {\n",
    "    'by_country' : 4,\n",
    "    'by_style' : 7,\n",
    "    'by_product' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = 'by_style' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = True\n",
    "saving_mode = True\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', problem_type, restoring_name)\n",
    "saving_path = os.path.join('models', problem_type, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "VALIDATION_BATCH = 16\n",
    "\n",
    "IMG_SIZE = 150\n",
    "CLASSES = classes_dict[problem_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, name, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "        \n",
    "def pooling_layer(X, kernel_size):\n",
    "    \"\"\"Perform max pooling\"\"\"\n",
    "    \n",
    "    return tf.nn.max_pool(X,\n",
    "                          ksize=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          strides=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Create a new fully connected layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # create the parameter structures     \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(n_inputs, n_neurons),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(n_neurons),\n",
    "                            name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "    \n",
    "    # decide the dataset input type\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "    # load training data from input queues     \n",
    "    images_trn, labels_trn = inputs(problem_type, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # load validation data from feed dictionary\n",
    "    images_val = tf.placeholder(tf.uint8, shape=[VALIDATION_BATCH, IMG_SIZE, IMG_SIZE, 3])\n",
    "    labels_val = tf.placeholder(tf.int32, shape=[VALIDATION_BATCH,])\n",
    "    \n",
    "    # choose the input\n",
    "    images, labels = tf.cond(is_training, lambda: (images_trn, labels_trn),\n",
    "                                          lambda: (images_val, labels_val))\n",
    "\n",
    "    # normalize the images     \n",
    "    images = (tf.cast(images, tf.float32) / 255.0)\n",
    "    # correct the labels     \n",
    "    labels = labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    #==================[ CONVOLUTIONAL LAYERS ]==================#\n",
    "    \n",
    "    images_conv_11 = conv_layer(images, 16, (5, 5), \"conv_11\", \"relu\")\n",
    "    images_conv_12 = conv_layer(images_conv_11, 16, (5, 5), \"conv_12\", \"relu\")\n",
    "    images_pool_1  = pooling_layer(images_conv_12, (3, 3))\n",
    "    \n",
    "    images_conv_21 = conv_layer(images_pool_1, 32, (3, 3), \"conv_21\", \"relu\")\n",
    "    images_conv_22 = conv_layer(images_conv_21, 32, (3, 3), \"conv_22\", \"relu\")\n",
    "    images_pool_2  = pooling_layer(images_conv_22, (3, 3))\n",
    "        \n",
    "    #==================[     DENSE LAYERS     ]==================#\n",
    "    \n",
    "    images_flatten = tf.contrib.layers.flatten(images_pool_2)\n",
    "    images_dense_1 = dense_layer(images_flatten, 128, \"dense_1\", \"relu\")\n",
    "    images_dense_2 = dense_layer(images_dense_1, 128, \"dense_2\", \"relu\")\n",
    "    \n",
    "    #==================[     OUTPUT LAYER     ]==================#\n",
    "    \n",
    "    logits = dense_layer(images_dense_2, CLASSES, \"logits\")\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "    #==================[     OPTIMIZATION     ]==================#\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    #==================[      EVALUATION      ]==================#\n",
    "    \n",
    "    _, accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions=tf.argmax(logits, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorboard old data\n",
    "try:\n",
    "    shutil.rmtree(os.path.join('tensorboard', problem_type))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    summary_conv_12 = tf.summary.image('conv_12', images_conv_12[:,:,:,:3])\n",
    "    summary_conv_22 = tf.summary.image('conv_22', images_conv_22[:,:,:,:3])\n",
    "    \n",
    "    train_loss = tf.summary.scalar('training_loss', loss)\n",
    "    val_loss = tf.summary.scalar('validation_loss', loss)\n",
    "    train_acc = tf.summary.scalar('training_accuracy', accuracy)\n",
    "    val_acc = tf.summary.scalar('validation_accuracy', accuracy)\n",
    "\n",
    "    convs_merged = tf.summary.merge([summary_conv_12, summary_conv_22])\n",
    "    train_merged = tf.summary.merge([train_loss, train_acc])\n",
    "    val_merged = tf.summary.merge([val_loss, val_acc])\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join('tensorboard', problem_type), sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "images_validation = np.load(os.path.join('data_' + problem_type, 'testing_data.dat'))\n",
    "labels_validation = np.load(os.path.join('data_' + problem_type, 'testing_labels.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    25 : Training >> loss = 1.86 accuracy = 19.94%  ||  Testing >> loss = 1.69 accuracy = 53.40%\n",
      "Step    50 : Training >> loss = 1.57 accuracy = 36.95%  ||  Testing >> loss = 1.59 accuracy = 35.72%\n",
      "Step    75 : Training >> loss = 1.52 accuracy = 32.48%  ||  Testing >> loss = 1.25 accuracy = 38.11%\n",
      "Step   100 : Training >> loss = 1.26 accuracy = 37.43%  ||  Testing >> loss = 1.33 accuracy = 40.62%\n",
      "Step   125 : Training >> loss = 1.25 accuracy = 39.27%  ||  Testing >> loss = 1.24 accuracy = 42.36%\n",
      "Step   150 : Training >> loss = 1.09 accuracy = 42.41%  ||  Testing >> loss = 1.13 accuracy = 44.76%\n",
      "Step   175 : Training >> loss = 1.12 accuracy = 45.04%  ||  Testing >> loss = 1.04 accuracy = 47.04%\n",
      "Step   200 : Training >> loss = 0.95 accuracy = 47.04%  ||  Testing >> loss = 1.02 accuracy = 48.60%\n",
      "Step   225 : Training >> loss = 1.11 accuracy = 48.62%  ||  Testing >> loss = 1.16 accuracy = 49.65%\n",
      "Step   250 : Training >> loss = 0.92 accuracy = 49.36%  ||  Testing >> loss = 1.07 accuracy = 50.55%\n",
      "Step   275 : Training >> loss = 0.88 accuracy = 50.85%  ||  Testing >> loss = 1.03 accuracy = 51.88%\n",
      "Step   300 : Training >> loss = 0.82 accuracy = 51.83%  ||  Testing >> loss = 1.07 accuracy = 52.66%\n",
      "Step   325 : Training >> loss = 0.93 accuracy = 52.69%  ||  Testing >> loss = 1.21 accuracy = 52.95%\n",
      "Step   350 : Training >> loss = 0.94 accuracy = 52.66%  ||  Testing >> loss = 1.04 accuracy = 53.32%\n",
      "Step   375 : Training >> loss = 0.94 accuracy = 53.25%  ||  Testing >> loss = 0.95 accuracy = 54.00%\n",
      "Step   400 : Training >> loss = 0.75 accuracy = 53.98%  ||  Testing >> loss = 1.21 accuracy = 54.17%\n",
      "Step   425 : Training >> loss = 0.66 accuracy = 54.22%  ||  Testing >> loss = 1.13 accuracy = 54.81%\n",
      "Step   450 : Training >> loss = 0.86 accuracy = 54.88%  ||  Testing >> loss = 1.13 accuracy = 55.34%\n",
      "Step   475 : Training >> loss = 0.84 accuracy = 55.23%  ||  Testing >> loss = 0.95 accuracy = 55.77%\n",
      "Step   500 : Training >> loss = 0.73 accuracy = 55.84%  ||  Testing >> loss = 0.97 accuracy = 56.20%\n",
      "Step   525 : Training >> loss = 0.66 accuracy = 56.26%  ||  Testing >> loss = 1.06 accuracy = 56.74%\n",
      "Step   550 : Training >> loss = 0.79 accuracy = 56.84%  ||  Testing >> loss = 0.99 accuracy = 57.24%\n",
      "Step   575 : Training >> loss = 0.68 accuracy = 57.18%  ||  Testing >> loss = 0.85 accuracy = 57.60%\n",
      "Step   600 : Training >> loss = 0.65 accuracy = 57.70%  ||  Testing >> loss = 0.97 accuracy = 58.07%\n",
      "Step   625 : Training >> loss = 0.60 accuracy = 58.12%  ||  Testing >> loss = 0.95 accuracy = 58.46%\n",
      "Step   650 : Training >> loss = 0.48 accuracy = 58.48%  ||  Testing >> loss = 1.11 accuracy = 58.82%\n",
      "Step   675 : Training >> loss = 0.50 accuracy = 58.94%  ||  Testing >> loss = 1.05 accuracy = 59.24%\n",
      "Step   700 : Training >> loss = 0.60 accuracy = 59.36%  ||  Testing >> loss = 1.04 accuracy = 59.64%\n",
      "Step   725 : Training >> loss = 0.54 accuracy = 59.68%  ||  Testing >> loss = 1.12 accuracy = 59.94%\n",
      "Step   750 : Training >> loss = 0.51 accuracy = 59.97%  ||  Testing >> loss = 1.05 accuracy = 60.30%\n",
      "Step   775 : Training >> loss = 0.47 accuracy = 60.39%  ||  Testing >> loss = 1.13 accuracy = 60.66%\n",
      "Step   800 : Training >> loss = 0.49 accuracy = 60.73%  ||  Testing >> loss = 1.06 accuracy = 61.02%\n",
      "Step   825 : Training >> loss = 0.44 accuracy = 61.05%  ||  Testing >> loss = 1.42 accuracy = 61.29%\n",
      "Step   850 : Training >> loss = 0.48 accuracy = 61.25%  ||  Testing >> loss = 0.92 accuracy = 61.53%\n",
      "Step   875 : Training >> loss = 0.49 accuracy = 61.64%  ||  Testing >> loss = 1.02 accuracy = 61.87%\n",
      "Step   900 : Training >> loss = 0.35 accuracy = 61.91%  ||  Testing >> loss = 1.10 accuracy = 62.18%\n",
      "Step   925 : Training >> loss = 0.40 accuracy = 62.21%  ||  Testing >> loss = 1.10 accuracy = 62.45%\n",
      "Step   950 : Training >> loss = 0.37 accuracy = 62.52%  ||  Testing >> loss = 1.17 accuracy = 62.69%\n",
      "Step   975 : Training >> loss = 0.29 accuracy = 62.73%  ||  Testing >> loss = 1.27 accuracy = 62.94%\n",
      "Step  1000 : Training >> loss = 0.31 accuracy = 62.98%  ||  Testing >> loss = 0.96 accuracy = 63.21%\n",
      "Step  1025 : Training >> loss = 0.37 accuracy = 63.28%  ||  Testing >> loss = 1.03 accuracy = 63.43%\n",
      "Step  1050 : Training >> loss = 0.49 accuracy = 63.43%  ||  Testing >> loss = 1.41 accuracy = 63.59%\n",
      "Step  1075 : Training >> loss = 0.41 accuracy = 63.56%  ||  Testing >> loss = 1.31 accuracy = 63.74%\n",
      "Step  1100 : Training >> loss = 0.44 accuracy = 63.79%  ||  Testing >> loss = 1.07 accuracy = 63.97%\n",
      "Step  1125 : Training >> loss = 0.41 accuracy = 63.96%  ||  Testing >> loss = 1.35 accuracy = 64.11%\n",
      "Step  1150 : Training >> loss = 0.30 accuracy = 64.11%  ||  Testing >> loss = 1.17 accuracy = 64.29%\n",
      "Step  1175 : Training >> loss = 0.28 accuracy = 64.35%  ||  Testing >> loss = 1.28 accuracy = 64.48%\n",
      "Step  1200 : Training >> loss = 0.37 accuracy = 64.53%  ||  Testing >> loss = 1.35 accuracy = 64.67%\n",
      "Step  1225 : Training >> loss = 0.30 accuracy = 64.67%  ||  Testing >> loss = 1.18 accuracy = 64.81%\n",
      "Step  1250 : Training >> loss = 0.26 accuracy = 64.86%  ||  Testing >> loss = 1.30 accuracy = 65.00%\n",
      "Step  1275 : Training >> loss = 0.25 accuracy = 65.05%  ||  Testing >> loss = 1.38 accuracy = 65.16%\n",
      "Step  1300 : Training >> loss = 0.27 accuracy = 65.18%  ||  Testing >> loss = 1.13 accuracy = 65.33%\n",
      "Done training -- epoch limit reached\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0\n",
    "    MILESTONE = 25\n",
    "    \n",
    "    min_val_loss = math.inf\n",
    "    max_val_acc = -math.inf\n",
    "    \n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    samples_count = 0\n",
    "    \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        _, loss_value, acc_value, summary, img_sum = sess.run(\n",
    "                [training_op, loss, accuracy, train_merged, convs_merged], feed_dict={\n",
    "            is_training : True,\n",
    "            images_val : images_validation[:VALIDATION_BATCH],\n",
    "            labels_val : labels_validation[:VALIDATION_BATCH]\n",
    "        })\n",
    "        \n",
    "        # save stats to log         \n",
    "        summary_writer.add_summary(summary, step)\n",
    "        summary_writer.add_summary(img_sum, step)\n",
    "        \n",
    "        acc_sum += acc_value\n",
    "        loss_sum += loss_value\n",
    "        samples_count += 1\n",
    "                        \n",
    "        if step % MILESTONE == 0:\n",
    "            \n",
    "            #===================[ TRAINING SCORE ]===================#\n",
    "            \n",
    "            training_acc = acc_sum / samples_count\n",
    "            training_loss = loss_sum / samples_count\n",
    "                \n",
    "            print(\"\\nStep {0:5d} : Training >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, training_loss, 100 * training_acc), flush=True, end=\"\")\n",
    "            \n",
    "            acc_sum = 0\n",
    "            loss_sum = 0\n",
    "            samples_count = 0  \n",
    "        \n",
    "            #===================[ TESTING SCORE ]===================#\n",
    "            \n",
    "            val_acc_sum = 0\n",
    "            val_loss_sum = 0\n",
    "            val_samples_count = 0  \n",
    "            \n",
    "            for i in range(VALIDATION_BATCH, len(labels_validation), VALIDATION_BATCH):\n",
    "                \n",
    "                loss_value, acc_value, summary = sess.run(\n",
    "                    [loss, accuracy, val_merged], feed_dict={\n",
    "                    is_training : False,\n",
    "                    images_val : images_validation[(i - VALIDATION_BATCH):i],\n",
    "                    labels_val : labels_validation[(i - VALIDATION_BATCH):i]\n",
    "                })\n",
    "                \n",
    "                val_acc_sum += acc_value\n",
    "                val_loss_sum += loss_value\n",
    "                val_samples_count += 1\n",
    "                \n",
    "            # save stats to log         \n",
    "            summary_writer.add_summary(summary, step)\n",
    "                \n",
    "            validation_acc = val_acc_sum / val_samples_count\n",
    "            validation_loss = val_loss_sum / val_samples_count\n",
    "            \n",
    "            print(\"  ||  Testing >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, validation_loss, 100 * validation_acc), flush=True, end=\"\")\n",
    "                        \n",
    "            saving_condition = (validation_loss < min_val_loss)\n",
    "\n",
    "            # save the model for later use         \n",
    "            if saving_mode and saving_condition:\n",
    "                saver.save(sess, saving_path)\n",
    "                \n",
    "            min_val_loss = min(min_val_loss, validation_loss)\n",
    "            max_val_acc  = max(max_val_acc,  validation_acc)\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
