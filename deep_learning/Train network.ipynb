{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "\n",
    "classes_dict = {\n",
    "    'by_country' : 4,\n",
    "    'by_style' : 7,\n",
    "    'by_product' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = 'by_style' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = True\n",
    "saving_mode = True\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', problem_type, restoring_name)\n",
    "saving_path = os.path.join('models', problem_type, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "VALIDATION_BATCH = 16\n",
    "\n",
    "IMG_SIZE = 150\n",
    "CLASSES = classes_dict[problem_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, name, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "        \n",
    "def pooling_layer(X, kernel_size):\n",
    "    \"\"\"Perform max pooling\"\"\"\n",
    "    \n",
    "    return tf.nn.max_pool(X,\n",
    "                          ksize=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          strides=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Create a new fully connected layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # create the parameter structures     \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(n_inputs, n_neurons),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(n_neurons),\n",
    "                            name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "    \n",
    "    # decide the dataset input type\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "    # load training data from input queues     \n",
    "    images_trn, labels_trn = inputs(problem_type, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # load validation data from feed dictionary\n",
    "    images_val = tf.placeholder(tf.uint8, shape=[VALIDATION_BATCH, IMG_SIZE, IMG_SIZE, 3])\n",
    "    labels_val = tf.placeholder(tf.int32, shape=[VALIDATION_BATCH,])\n",
    "    \n",
    "    # choose the input\n",
    "    images, labels = tf.cond(is_training, lambda: (images_trn, labels_trn),\n",
    "                                          lambda: (images_val, labels_val))\n",
    "\n",
    "    # normalize the images     \n",
    "    images = (tf.cast(images, tf.float32) / 255.0)\n",
    "    # correct the labels     \n",
    "    labels = labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    #==================[ CONVOLUTIONAL LAYERS ]==================#\n",
    "    \n",
    "    images_conv_11 = conv_layer(images, 16, (5, 5), \"conv_11\", \"relu\")\n",
    "    images_conv_12 = conv_layer(images_conv_11, 16, (5, 5), \"conv_12\", \"relu\")\n",
    "    images_pool_1  = pooling_layer(images_conv_12, (3, 3))\n",
    "    \n",
    "    images_conv_21 = conv_layer(images_pool_1, 32, (3, 3), \"conv_21\", \"relu\")\n",
    "    images_conv_22 = conv_layer(images_conv_21, 32, (3, 3), \"conv_22\", \"relu\")\n",
    "    images_pool_2  = pooling_layer(images_conv_22, (3, 3))\n",
    "        \n",
    "    #==================[     DENSE LAYERS     ]==================#\n",
    "    \n",
    "    images_flatten = tf.contrib.layers.flatten(images_pool_2)\n",
    "    images_dense_1 = dense_layer(images_flatten, 128, \"dense_1\", \"relu\")\n",
    "    images_dense_2 = dense_layer(images_dense_1, 128, \"dense_2\", \"relu\")\n",
    "    \n",
    "    #==================[     OUTPUT LAYER     ]==================#\n",
    "    \n",
    "    logits = dense_layer(images_dense_2, CLASSES, \"logits\")\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "    #==================[     OPTIMIZATION     ]==================#\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    #==================[      EVALUATION      ]==================#\n",
    "    \n",
    "    _, accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions=tf.argmax(logits, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\by_style\\first_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorboard old data\n",
    "try:\n",
    "    shutil.rmtree(os.path.join('tensorboard', problem_type))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    summary_conv_12 = tf.summary.image('conv_12', images_conv_12[:,:,:,:3])\n",
    "    summary_conv_22 = tf.summary.image('conv_22', images_conv_22[:,:,:,:3])\n",
    "    \n",
    "    train_loss = tf.summary.scalar('training_loss', loss)\n",
    "    val_loss = tf.summary.scalar('validation_loss', loss)\n",
    "    train_acc = tf.summary.scalar('training_accuracy', accuracy)\n",
    "    val_acc = tf.summary.scalar('validation_accuracy', accuracy)\n",
    "\n",
    "    convs_merged = tf.summary.merge([summary_conv_12, summary_conv_22])\n",
    "    train_merged = tf.summary.merge([train_loss, train_acc])\n",
    "    val_merged = tf.summary.merge([val_loss, val_acc])\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join('tensorboard', problem_type), sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "images_validation = np.load(os.path.join('data_' + problem_type, 'testing_data.dat'))\n",
    "labels_validation = np.load(os.path.join('data_' + problem_type, 'testing_labels.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    25 : Training >> loss = 0.58 accuracy = 75.98%  ||  Testing >> loss = 0.99 accuracy = 80.50%\n",
      "Step    50 : Training >> loss = 0.86 accuracy = 69.23%  ||  Testing >> loss = 1.11 accuracy = 69.17%\n",
      "Step    75 : Training >> loss = 0.70 accuracy = 65.02%  ||  Testing >> loss = 0.83 accuracy = 67.33%\n",
      "Step   100 : Training >> loss = 0.59 accuracy = 66.87%  ||  Testing >> loss = 1.11 accuracy = 67.95%\n",
      "Step   125 : Training >> loss = 0.63 accuracy = 66.69%  ||  Testing >> loss = 1.08 accuracy = 67.23%\n",
      "Step   150 : Training >> loss = 0.56 accuracy = 66.38%  ||  Testing >> loss = 1.08 accuracy = 67.28%\n",
      "Step   175 : Training >> loss = 0.47 accuracy = 66.87%  ||  Testing >> loss = 1.32 accuracy = 67.55%\n",
      "Step   200 : Training >> loss = 0.59 accuracy = 66.88%  ||  Testing >> loss = 1.14 accuracy = 67.32%\n",
      "Step   225 : Training >> loss = 0.48 accuracy = 66.88%  ||  Testing >> loss = 1.05 accuracy = 67.54%\n",
      "Step   250 : Training >> loss = 0.46 accuracy = 67.40%  ||  Testing >> loss = 1.19 accuracy = 67.79%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-08e43c219df4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mis_training\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mimages_val\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mimages_validation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mVALIDATION_BATCH\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mlabels_val\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlabels_validation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mVALIDATION_BATCH\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         })\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\work\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step = 0\n",
    "    MILESTONE = 25\n",
    "    \n",
    "    min_val_loss = math.inf\n",
    "    max_val_acc = -math.inf\n",
    "    \n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    samples_count = 0\n",
    "    \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        _, loss_value, acc_value, summary, img_sum = sess.run(\n",
    "                [training_op, loss, accuracy, train_merged, convs_merged], feed_dict={\n",
    "            is_training : True,\n",
    "            images_val : images_validation[:VALIDATION_BATCH],\n",
    "            labels_val : labels_validation[:VALIDATION_BATCH]\n",
    "        })\n",
    "        \n",
    "        # save stats to log         \n",
    "        summary_writer.add_summary(summary, step)\n",
    "        summary_writer.add_summary(img_sum, step)\n",
    "        \n",
    "        acc_sum += acc_value\n",
    "        loss_sum += loss_value\n",
    "        samples_count += 1\n",
    "                        \n",
    "        if step % MILESTONE == 0:\n",
    "            \n",
    "            #===================[ TRAINING SCORE ]===================#\n",
    "            \n",
    "            training_acc = acc_sum / samples_count\n",
    "            training_loss = loss_sum / samples_count\n",
    "                \n",
    "            print(\"\\nStep {0:5d} : Training >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, training_loss, 100 * training_acc), flush=True, end=\"\")\n",
    "            \n",
    "            acc_sum = 0\n",
    "            loss_sum = 0\n",
    "            samples_count = 0  \n",
    "        \n",
    "            #===================[ TESTING SCORE ]===================#\n",
    "            \n",
    "            val_acc_sum = 0\n",
    "            val_loss_sum = 0\n",
    "            val_samples_count = 0  \n",
    "            \n",
    "            for i in range(VALIDATION_BATCH, len(labels_validation), VALIDATION_BATCH):\n",
    "                \n",
    "                loss_value, acc_value, summary = sess.run(\n",
    "                    [loss, accuracy, val_merged], feed_dict={\n",
    "                    is_training : False,\n",
    "                    images_val : images_validation[(i - VALIDATION_BATCH):i],\n",
    "                    labels_val : labels_validation[(i - VALIDATION_BATCH):i]\n",
    "                })\n",
    "                \n",
    "                val_acc_sum += acc_value\n",
    "                val_loss_sum += loss_value\n",
    "                val_samples_count += 1\n",
    "                \n",
    "            # save stats to log         \n",
    "            summary_writer.add_summary(summary, step)\n",
    "                \n",
    "            validation_acc = val_acc_sum / val_samples_count\n",
    "            validation_loss = val_loss_sum / val_samples_count\n",
    "            \n",
    "            print(\"  ||  Testing >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, validation_loss, 100 * validation_acc), flush=True, end=\"\")\n",
    "                        \n",
    "            saving_condition = (validation_loss < min_val_loss)\n",
    "\n",
    "            # save the model for later use         \n",
    "            if saving_mode and saving_condition:\n",
    "                saver.save(sess, saving_path)\n",
    "                \n",
    "            min_val_loss = min(min_val_loss, validation_loss)\n",
    "            max_val_acc  = max(max_val_acc,  validation_acc)\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work]",
   "language": "python",
   "name": "conda-env-work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
