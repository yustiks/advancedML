{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from read_data import *\n",
    "\n",
    "classes_dict = {\n",
    "    'by_country' : 4,\n",
    "    'by_style' : 7,\n",
    "    'by_product' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type = 'by_style' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set saving / restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "restoring_mode = False\n",
    "saving_mode = False\n",
    "\n",
    "restoring_name = 'first_model.ckpt'\n",
    "saving_name = 'first_model.ckpt'\n",
    "\n",
    "restoring_path = os.path.join('models', problem_type, restoring_name)\n",
    "saving_path = os.path.join('models', problem_type, saving_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "VALIDATION_BATCH = 16\n",
    "\n",
    "IMG_SIZE = 150\n",
    "CLASSES = classes_dict[problem_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, filters, filter_size, name, activation=None):\n",
    "    \"\"\"Create a new convolution layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        in_channels = int(X.get_shape()[3])\n",
    "        init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    \n",
    "        # create the parameter structures         \n",
    "        W = tf.get_variable(initializer=init, \n",
    "                            shape=(filter_size[0], filter_size[1],\n",
    "                                   in_channels, filters),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(filters),\n",
    "                            name=\"biases\")\n",
    "        \n",
    "        # perform convolution and add bias\n",
    "        conv = tf.nn.conv2d(X, W, strides=(1, 1, 1, 1), padding=\"SAME\")\n",
    "        z = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # activation function\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "\n",
    "        \n",
    "def pooling_layer(X, kernel_size):\n",
    "    \"\"\"Perform max pooling\"\"\"\n",
    "    \n",
    "    return tf.nn.max_pool(X,\n",
    "                          ksize=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          strides=(1, kernel_size[0], kernel_size[1], 1),\n",
    "                          padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Create a new fully connected layer with Xavier initializer\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        \n",
    "        # create Xavier initializer node \n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # create the parameter structures     \n",
    "        W = tf.get_variable(initializer=init,\n",
    "                            shape=(n_inputs, n_neurons),\n",
    "                            name=\"weights\")\n",
    "        b = tf.get_variable(initializer=tf.zeros(n_neurons),\n",
    "                            name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and deploy it on a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #==================[ READ AND PROCESS THE INPUT ]==================#\n",
    "    \n",
    "    # decide the dataset input type\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "    # load training data from input queues     \n",
    "    images_trn, labels_trn = inputs(problem_type, BATCH_SIZE, EPOCHS)\n",
    "    \n",
    "    # load validation data from feed dictionary\n",
    "    images_val = tf.placeholder(tf.uint8, shape=[VALIDATION_BATCH, IMG_SIZE, IMG_SIZE, 3])\n",
    "    labels_val = tf.placeholder(tf.int32, shape=[VALIDATION_BATCH,])\n",
    "    \n",
    "    # choose the input\n",
    "    images, labels = tf.cond(is_training, lambda: (images_trn, labels_trn),\n",
    "                                          lambda: (images_val, labels_val))\n",
    "\n",
    "    # normalize the images     \n",
    "    images = (tf.cast(images, tf.float32) / 255.0)\n",
    "    # correct the labels     \n",
    "    labels = labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\apps\\anaconda2\\envs\\work\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    #==================[ CONVOLUTIONAL LAYERS ]==================#\n",
    "    \n",
    "    images_conv_11 = conv_layer(images, 16, (5, 5), \"conv_11\", \"relu\")\n",
    "    images_conv_12 = conv_layer(images_conv_11, 16, (5, 5), \"conv_12\", \"relu\")\n",
    "    images_pool_1  = pooling_layer(images_conv_12, (3, 3))\n",
    "    \n",
    "    images_conv_21 = conv_layer(images_pool_1, 32, (3, 3), \"conv_21\", \"relu\")\n",
    "    images_conv_22 = conv_layer(images_conv_21, 32, (3, 3), \"conv_22\", \"relu\")\n",
    "    images_pool_2  = pooling_layer(images_conv_22, (3, 3))\n",
    "        \n",
    "    #==================[     DENSE LAYERS     ]==================#\n",
    "    \n",
    "    images_flatten = tf.contrib.layers.flatten(images_pool_2)\n",
    "    images_dense_1 = dense_layer(images_flatten, 128, \"dense_1\", \"relu\")\n",
    "    images_dense_2 = dense_layer(images_dense_1, 128, \"dense_2\", \"relu\")\n",
    "    \n",
    "    #==================[     OUTPUT LAYER     ]==================#\n",
    "    \n",
    "    logits = dense_layer(images_dense_2, CLASSES, \"logits\")\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "    #==================[     OPTIMIZATION     ]==================#\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "    #==================[      EVALUATION      ]==================#\n",
    "    \n",
    "    _, accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions=tf.argmax(logits, -1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session and start the threads for input queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the session saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# create a session for running operations in the graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# create the variable initializers\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "# initialize the variables\n",
    "sess.run(init_op)\n",
    "\n",
    "if restoring_mode:\n",
    "    # previously saved model is restored\n",
    "    saver.restore(sess, restoring_path)\n",
    "    \n",
    "# start input enqueue threads.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear tensorboard old data\n",
    "try:\n",
    "    shutil.rmtree(os.path.join('tensorboard', problem_type))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    summary_conv_12 = tf.summary.image('conv_12', images_conv_12[:,:,:,:3])\n",
    "    summary_conv_22 = tf.summary.image('conv_22', images_conv_22[:,:,:,:3])\n",
    "    \n",
    "    train_loss = tf.summary.scalar('training_loss', loss)\n",
    "    val_loss = tf.summary.scalar('validation_loss', loss)\n",
    "    train_acc = tf.summary.scalar('training_accuracy', accuracy)\n",
    "    val_acc = tf.summary.scalar('validation_accuracy', accuracy)\n",
    "\n",
    "    convs_merged = tf.summary.merge([summary_conv_12, summary_conv_22])\n",
    "    train_merged = tf.summary.merge([train_loss, train_acc])\n",
    "    val_merged = tf.summary.merge([val_loss, val_acc])\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(\n",
    "        os.path.join('tensorboard', problem_type), sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation data\n",
    "images_validation = np.load(os.path.join('data_' + problem_type, 'testing_data.dat'))\n",
    "labels_validation = np.load(os.path.join('data_' + problem_type, 'testing_labels.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    step = 0\n",
    "    MILESTONE = 25\n",
    "    \n",
    "    min_val_loss = math.inf\n",
    "    max_val_acc = -math.inf\n",
    "    \n",
    "    acc_sum = 0\n",
    "    loss_sum = 0\n",
    "    samples_count = 0\n",
    "    \n",
    "    # feed data until the epoch limit is reached     \n",
    "    while not coord.should_stop():\n",
    "\n",
    "        step += 1\n",
    "        \n",
    "        _, loss_value, acc_value, summary, img_sum = sess.run(\n",
    "                [training_op, loss, accuracy, train_merged, convs_merged], feed_dict={\n",
    "            is_training : True,\n",
    "            images_val : images_validation[:VALIDATION_BATCH],\n",
    "            labels_val : labels_validation[:VALIDATION_BATCH]\n",
    "        })\n",
    "        \n",
    "        # save stats to log         \n",
    "        summary_writer.add_summary(summary, step)\n",
    "        summary_writer.add_summary(img_sum, step)\n",
    "        \n",
    "        acc_sum += acc_value\n",
    "        loss_sum += loss_value\n",
    "        samples_count += 1\n",
    "                        \n",
    "        if step % MILESTONE == 0:\n",
    "            \n",
    "            #===================[ TRAINING SCORE ]===================#\n",
    "            \n",
    "            training_acc = acc_sum / samples_count\n",
    "            training_loss = loss_sum / samples_count\n",
    "                \n",
    "            print(\"\\nStep {0:5d} : Training >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, training_loss, 100 * training_acc), flush=True, end=\"\")\n",
    "            \n",
    "            acc_sum = 0\n",
    "            loss_sum = 0\n",
    "            samples_count = 0  \n",
    "        \n",
    "            #===================[ TESTING SCORE ]===================#\n",
    "            \n",
    "            val_acc_sum = 0\n",
    "            val_loss_sum = 0\n",
    "            val_samples_count = 0  \n",
    "            \n",
    "            for i in range(VALIDATION_BATCH, len(labels_validation), VALIDATION_BATCH):\n",
    "                \n",
    "                loss_value, acc_value, summary = sess.run(\n",
    "                    [loss, accuracy, val_merged], feed_dict={\n",
    "                    is_training : False,\n",
    "                    images_val : images_validation[(i - VALIDATION_BATCH):i],\n",
    "                    labels_val : labels_validation[(i - VALIDATION_BATCH):i]\n",
    "                })\n",
    "                \n",
    "                val_acc_sum += acc_value\n",
    "                val_loss_sum += loss_value\n",
    "                val_samples_count += 1\n",
    "                \n",
    "            # save stats to log         \n",
    "            summary_writer.add_summary(summary, step)\n",
    "                \n",
    "            validation_acc = val_acc_sum / val_samples_count\n",
    "            validation_loss = val_loss_sum / val_samples_count\n",
    "            \n",
    "            print(\"  ||  Testing >> loss = {1:.2f} accuracy = {2:5.2f}%\" \\\n",
    "                  .format(step, validation_loss, 100 * validation_acc), flush=True, end=\"\")\n",
    "                        \n",
    "            saving_condition = (validation_loss < min_val_loss)\n",
    "\n",
    "            # save the model for later use         \n",
    "            if saving_mode and saving_condition:\n",
    "                saver.save(sess, saving_path)\n",
    "                \n",
    "            min_val_loss = min(min_val_loss, validation_loss)\n",
    "            max_val_acc  = max(max_val_acc,  validation_acc)\n",
    "        \n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    \n",
    "    print('\\nDone training -- epoch limit reached\\n')\n",
    "    \n",
    "finally:\n",
    "    \n",
    "    # when done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "    # wait for threads to finish\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
